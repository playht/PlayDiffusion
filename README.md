# PlayAI InPainter

## Installation

Requires OPENAI_API_KEY env var

1. Set up virtualenv: `python3.11 -m venv .venv`
2. Activate virutalenv: `source .venv/bin/activate`
3. Install package and dependencies: `pip install '.[demo]'`
4. Run demo: `python demo/gradio-demo.py`


## Introduction


Autoregressive transformer models have proven highly effective for synthesizing speech from text. However, they face a significant limitation: **modifying portions of the generated audio —known as inpainting— or removing them without leaving discontinuity artifacts is beyond their standard capabilities**. Thus different approaches are needed for more general speech editing tools. Consider the sentence:

> *“The answer is out there, Neo. Go grab it!”*
> 

Now suppose you want to change “Neo” to “Trinity” *after* generation. With traditional AR models, your options are limited:

- **Regenerate the entire sentence**, which is computationally expensive and often leads to variations in prosody or speech rhythm.
- **Replace just the word “Neo”**, which results in noticeable artifacts or mismatches at word boundaries.
- **Regenerate from a midway point**, e.g., from “Trinity. Go grab it!”, but this has the potential of changing the prosody of the unedited part ("Go grab it"), creating unwanted variations in the speech rhythm.

All of these approaches compromise both the coherence and naturalness of the audio.


## PlayAI Discrete Diffusion

At [Play.AI](http://play.ai/), we’ve addressed this problem with a **novel diffusion-based approach for audio speech editing**. Here’s how it works:

1. First, we encode an audio sequence into a discrete space, converting the waveform into a more compact representation. Each unit in this representation is called a **token**. This process works with both real speech and audio generated by Text-to-Speech models.
2. When a segment needs to be modified, we **mask** that portion of the audio.
3. A **diffusion model**, conditioned on the updated text is used to **denoise** the masked region.
    1. The surrounding context is preserved seamlessly, ensuring smooth transitions and consistent speaker characteristics.
4. The resulting output token sequence is then transformed back to a speech waveform using our BigVGAN decoder model.

By using a **non-autoregressive diffusion model**, we can better maintain context at the edit boundaries—resulting in high-quality, coherent audio edits. This marks a significant step forward in audio editing capabilities and paves the way for dynamic, fine-grained speech modification.

The full process is outlined in Figure 1.

![Figure 1](./assets/DiscreteDiffusion.png)
